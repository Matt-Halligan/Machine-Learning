# Machine-Learning
I explore the foundations of machine learning and the sub-field of deep learning utilizing a mix of pre-made and custom mathematics for machine learning libraries, synthesizing seminal research including "Attention is All You Need," and building and adapting system workflows to train, deploy, and evolve ML/ML Ops architectures.

## Table of Contents

- [Introduction](#introduction)
- [Mathematics for Machine Learning](#mathematics-for-machine-learning)
- [Seminal Research](#seminal-research)
- [ML Architectures](#ml-architectures)
- [ML Ops Architectures](#ml-ops-architectures)
- [Getting Started](#getting-started)
- [Project Structure](#project-structure)
- [License](#license)
- [References](#references)

---

## Introduction

This repository serves as both a learning and development platform for various machine learning systems and architectures, particularly those based on transformer models (such as BERT, GPT, and ViT). The aim is to not only study theoretical frameworks but also to build practical applications, from simple model fine-tuning to advanced ML Ops pipelines for large-scale deployments.

---

## Mathematics for Machine Learning
[TBD]

---

## Seminal / Foundational Research

### "Attention Is All You Need" - Vaswani et al. (2017)
- **Link to Paper**: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

### "Formal Algorithms for Transformers" - Phuong et al. (2022)
- **Link to Paper**: [Formal Algorithms for Transformers](https://arxiv.org/abs/2207.09238)

---

## ML Architectures

This section delves into various machine learning architectures

---

## ML Ops Architectures

ML Ops focuses on the deployment, monitoring, and continuous integration of machine learning models in production environments.

- **Model Deployment**: Tools such as [TBD].
- **Scaling and Efficiency**: Techniques for distributed training, quantization, and model compression.
- **Model Monitoring**: Logging, performance metrics, and drift detection for model evaluation post-deployment.

---

## Getting Started

### Prerequisites

To run the code in this repository, you will need the following installed:

-

### Installation

1. ...

## Project Structure

The repository is organized as follows:

- `examples/`: Example scripts to demonstrate model training, fine-tuning, and inference.
- `models/`: Implementation of various machine learning models and architectures.
- `notebooks/`: Jupyter notebooks with hands-on exploration and experimentation.
- `scripts/`: Utility scripts for data preprocessing, model training, and evaluation.
- `docs/`: Documentation and notes on theoretical aspects, algorithm explanations, and ML ops workflows.
- `requirements.txt`: List of dependencies required to run the code.

---


## References

- **"Attention Is All You Need" (Vaswani et al., 2017)**: [Link to paper via arXiv](https://arxiv.org/abs/1706.03762)
- **"Formal Algorithms for Transformers" (Phuong et al., 2022)**: [Link to paper via arXiv](https://arxiv.org/abs/2207.09238)
- **Mathematics for Machine Learning**: Deisenroth, M. P., Faisal, A. A., & Ong, C. S. (2020). *Cambridge University Press*.
    - [Book Link via Github.io](https://mml-book.github.io/book/mml-book.pdf)
